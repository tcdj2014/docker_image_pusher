import time
import requests
from requests.adapters import HTTPAdapter
try:
    from urllib3.util.retry import Retry
except ImportError:
    from requests.packages.urllib3.util.retry import Retry
import pymysql  # éœ€è¦å…ˆ pip install PyMySQL
import redis  # éœ€è¦å…ˆ pip install redis
import logging
from datetime import datetime, timedelta
import yaml
import os

# --- æ—¥å¿—é…ç½® ---
# é…ç½®æ—¥å¿—æ ¼å¼ï¼ŒåŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œå¯é€‰çš„æ—¥å¿—æ–‡ä»¶
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

logger = logging.getLogger(__name__)

# --- åŠ è½½é…ç½® ---
def load_config():
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®ï¼Œå¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
    """
    config_path = 'config.yaml'
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    else:
        # é»˜è®¤é…ç½®
        config = {
            "archive_config": {
                "increment_value": 10000,
                "default_min_id_value": 10000,
                "total_iterations": 1000
            },
            "database": {
                "host": "x",
                "port": 30001,
                "user": "x",
                "password": "x",
                "database": "x",
                "charset": "utf8mb4"
            },
            "redis": {
                "host": "x",
                "port": 36379,
                "password": "xxx",
                "db": 0,
                "decode_responses": True
            },
            "api": {
                "url": "https://xxx"
            },
            "lock": {
                "key": "x",
                "wait_seconds": 30
            },
            "thread_pool": {
                "max_workers": 5
            },
            "retry_policy": {
                "total": 3,
                "backoff_factor": 1,
                "status_forcelist": [429, 500, 502, 503, 504]
            },
            "connection_pool": {
                "pool_connections": 10,
                "pool_maxsize": 20
            }
        }
    
    return config

config = load_config()

# ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®å€¼
ARCHIVE_INCREMENT_VALUE = config['archive_config']['increment_value']
DEFAULT_MIN_ID_VALUE = config['archive_config']['default_min_id_value']
total_iterations = config['archive_config']['total_iterations']

# è¯·åŠ¡å¿…ä¿®æ”¹ä»¥ä¸‹æ•°æ®åº“è¿æ¥å‚æ•°ä¸ºæ‚¨å®é™…çš„æ•°æ®åº“ä¿¡æ¯
DB_CONFIG = {
    'host': config['database']['host'],
    'port': config['database']['port'],
    'user': config['database']['user'],
    'password': config['database']['password'],
    'database': config['database']['database'],
    'charset': config['database']['charset']
}

# è¯·åŠ¡å¿…ä¿®æ”¹ä»¥ä¸‹ Redis è¿æ¥å‚æ•°ä¸ºæ‚¨å®é™…çš„ Redis ä¿¡æ¯
REDIS_CONFIG = {
    'host': config['redis']['host'],
    'port': config['redis']['port'],
    'password': config['redis']['password'],
    'db': config['redis']['db'],
    'decode_responses': config['redis']['decode_responses']
}

API_URL = config['api']['url']
LOCK_KEY = config['lock']['key']  # Redis ä¸­çš„é”é”®å
WAIT_SECONDS_FOR_LOCK_CHECK = config['lock']['wait_seconds']  # æ£€æŸ¥ Redis é”æ—¶ï¼Œæ¯æ¬¡è½®è¯¢çš„é—´éš”æ—¶é—´

# åˆ›å»ºå…¨å±€ä¼šè¯å¯¹è±¡ï¼Œå¯ç”¨è¿æ¥æ± å’Œé•¿è¿æ¥
session = requests.Session()

# é…ç½®é‡è¯•ç­–ç•¥
retry_strategy = Retry(
    total=config['retry_policy']['total'],
    backoff_factor=config['retry_policy']['backoff_factor'],
    status_forcelist=config['retry_policy']['status_forcelist'],
)

# é…ç½®é€‚é…å™¨ï¼Œåº”ç”¨é‡è¯•ç­–ç•¥
adapter = HTTPAdapter(
    pool_connections=config['connection_pool']['pool_connections'],  # è¿æ¥æ± çš„è¿æ¥æ•°
    pool_maxsize=config['connection_pool']['pool_maxsize'],      # æœ€å¤§è¿æ¥æ•°
    max_retries=retry_strategy
)

# ä¸ºHTTPå’ŒHTTPSè¯·æ±‚æŒ‚è½½é€‚é…å™¨
session.mount("http://", adapter)
session.mount("https://", adapter)

# çº¿ç¨‹æ± å¤§å°é…ç½®
MAX_WORKERS = config['thread_pool']['max_workers']  # æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°

# --- é…ç½®åŠ è½½å®Œæˆ ---


def check_long_connection_support(url):
    """
    æ£€æŸ¥æŒ‡å®šURLæ˜¯å¦æ”¯æŒé•¿è¿æ¥
    """
    try:
        # å‘é€ä¸€ä¸ªHEADè¯·æ±‚æ¥æ£€æŸ¥Connectionå¤´éƒ¨
        response = session.head(url, timeout=10)
        
        # æ£€æŸ¥å“åº”å¤´éƒ¨ä¸­æ˜¯å¦æ”¯æŒé•¿è¿æ¥
        connection_header = response.headers.get('Connection', '').lower()
        keep_alive_header = response.headers.get('Keep-Alive', '')
        
        # HTTP/1.1 é»˜è®¤æ”¯æŒé•¿è¿æ¥ï¼Œé™¤éæ˜ç¡®æŒ‡å®š Connection: close
        is_http11 = response.version == 11 if hasattr(response, 'version') else True
        is_close = connection_header == 'close'
        
        supports_keepalive = (is_http11 and not is_close) or connection_header == 'keep-alive'
        
        logger.info(f"  ğŸ” é•¿è¿æ¥æ£€æŸ¥ç»“æœ: æ”¯æŒé•¿è¿æ¥={supports_keepalive}, Connectionå¤´éƒ¨='{connection_header}', Keep-Alive='{keep_alive_header}'")
        
        return supports_keepalive
    except Exception as e:
        logger.error(f"  âŒ æ£€æŸ¥é•¿è¿æ¥æ”¯æŒæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False

def initialize_and_update():
    """
    åˆå§‹åŒ–å‡½æ•°ï¼šæ ¹æ®è¡¨åå’Œttx_archive_rule_termä¸­çš„fieldã€operatorã€valueæ¡ä»¶ï¼ŒæŸ¥è¯¢å‡ºMIN(id)å¹¶æ›´æ–°åˆ°å½’æ¡£è§„åˆ™ä¸­
    åŒæ—¶è€ƒè™‘æ—¶é—´æ¡ä»¶ï¼šè¡¨.created < ttx_archive_rule_header.archiveDaysBefore
    """
    db_connection = None
    redis_client = None

    try:
        # è¿æ¥æ•°æ®åº“
        logger.info("æ­£åœ¨è¿æ¥æ•°æ®åº“...")
        db_connection = pymysql.connect(**DB_CONFIG)
        db_cursor = db_connection.cursor()

        # è¿æ¥ Redis
        logger.info("æ­£åœ¨è¿æ¥ Redis...")
        redis_client = redis.Redis(**REDIS_CONFIG)
        # å°è¯•æ‰§è¡Œä¸€ä¸ªç®€å•çš„ Redis å‘½ä»¤æ¥æµ‹è¯•è¿æ¥
        redis_client.ping()
        logger.info("Redis è¿æ¥æˆåŠŸã€‚")

        # æŸ¥è¯¢æ‰€æœ‰ autoArchive=1 çš„è¡¨å¤´ä¿¡æ¯ï¼ŒåŒ…æ‹¬å½’æ¡£å¤©æ•°è®¾ç½®
        logger.info("æ­£åœ¨æŸ¥è¯¢ ttx_archive_rule_header è¡¨ä¸­ autoArchive=1 çš„è®°å½•...")
        query_sql = "SELECT id, tableName, archiveDaysBefore FROM ttx_archive_rule_header WHERE autoArchive=1"
        db_cursor.execute(query_sql)
        table_records = db_cursor.fetchall()

        if not table_records:
            logger.warning("æœªæ‰¾åˆ° autoArchive=1 çš„è¡¨è®°å½•ï¼Œç¨‹åºé€€å‡ºã€‚")
            return

        logger.info(f"å…±æŸ¥è¯¢åˆ° {len(table_records)} ä¸ªéœ€è¦å½’æ¡£çš„è¡¨:")
        for record in table_records:
            logger.info(f"  - ID: {record[0]}, TableName: {record[1]}, ArchiveDaysBefore: {record[2]}")

        # å¯¹æ¯ä¸ªè¡¨è¿›è¡Œåˆå§‹åŒ–æ“ä½œ
        for header_id, table_name, archive_days_before in table_records:
            logger.info(f"\n--- å¼€å§‹å¤„ç†è¡¨ {table_name} (ID: {header_id}, å½’æ¡£å¤©æ•°: {archive_days_before}) ---")

            # æŸ¥è¯¢è¯¥è¡¨å¤´å¯¹åº”çš„è§„åˆ™æ¡ä»¶
            rule_query = "SELECT field, operator, value FROM ttx_archive_rule_term WHERE headerId=%s;"
            db_cursor.execute(rule_query, (header_id,))
            rules = db_cursor.fetchall()

            # è®¡ç®—å½’æ¡£æ—¥æœŸé˜ˆå€¼ï¼Œå¹¶å°†æ—¶åˆ†ç§’è°ƒæ•´ä¸º 00:00:00
            if archive_days_before is not None and archive_days_before > 0:
                archive_date_raw = datetime.now() - timedelta(days=archive_days_before)
                # è·å–æ—¥æœŸéƒ¨åˆ†ï¼Œå¹¶ç»„åˆä¸ºå½“å¤©çš„ 00:00:00
                archive_date_threshold = datetime.combine(archive_date_raw.date(), datetime.min.time())
                logger.info(f"    å½’æ¡£æ—¥æœŸé˜ˆå€¼: {archive_date_threshold.strftime('%Y-%m-%d %H:%M:%S')}")
            else:
                logger.info(f"    æœªè®¾ç½®å½’æ¡£å¤©æ•°ï¼Œè®¾ç½®é»˜è®¤æ—¶é—´æ¡ä»¶180å¤©")
                archive_date_raw_default = datetime.now() - timedelta(days=180)
                # è·å–æ—¥æœŸéƒ¨åˆ†ï¼Œå¹¶ç»„åˆä¸ºå½“å¤©çš„ 00:00:00
                archive_date_threshold = datetime.combine(archive_date_raw_default.date(), datetime.min.time())

            if not rules:
                logger.warning(
                    f"  è¡¨ {table_name} (ID: {header_id}) æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è§„åˆ™æ¡ä»¶ï¼Œè®¾ç½®é»˜è®¤å€¼{DEFAULT_MIN_ID_VALUE}")

                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ id < X çš„è§„åˆ™ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ’å…¥
                check_sql = "SELECT value FROM ttx_archive_rule_term WHERE headerId=%s AND field='id' AND operator='<';"
                db_cursor.execute(check_sql, (header_id,))
                existing_result = db_cursor.fetchone()

                if existing_result:
                    # å¦‚æœå·²æœ‰è§„åˆ™ï¼Œæ›´æ–°ä¸ºé»˜è®¤å€¼
                    update_sql = f"UPDATE ttx_archive_rule_term SET `value`={DEFAULT_MIN_ID_VALUE} WHERE headerId=%s AND field='id' AND operator='<';"
                    db_cursor.execute(update_sql, (header_id,))
                    affected_rows = db_cursor.rowcount
                    logger.info(f"    âœ“ æ›´æ–°äº† {affected_rows} æ¡è®°å½•ï¼Œvalue è®¾ç½®ä¸ºé»˜è®¤å€¼ {DEFAULT_MIN_ID_VALUE}")
                else:
                    # å¦‚æœæ²¡æœ‰è§„åˆ™ï¼Œæ’å…¥æ–°çš„è§„åˆ™
                    insert_sql = """
                    INSERT INTO ttx_archive_rule_term (headerId, field, operator, value, created, lastUpdated, createdBy, lastUpdatedBy)
                    VALUES (%s, 'id', '<', %s, NOW(), NOW(), 'INIT_SYSTEM', 'INIT_SYSTEM');
                    """
                    db_cursor.execute(insert_sql, (header_id, DEFAULT_MIN_ID_VALUE))
                    affected_rows = db_cursor.rowcount
                    logger.info(f"    âœ“ æ’å…¥äº† {affected_rows} æ¡è®°å½•ï¼Œvalue è®¾ç½®ä¸ºé»˜è®¤å€¼ {DEFAULT_MIN_ID_VALUE}")

                continue  # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè¡¨

            # æ„å»ºåŠ¨æ€WHEREæ¡ä»¶ï¼Œæ’é™¤field='id'çš„è§„åˆ™
            where_conditions = []
            params = []

            for field, operator, value in rules:
                if field.lower() != 'id':
                    # æ·»åŠ åˆ°WHEREæ¡ä»¶
                    where_conditions.append(f"`{field}` {operator} %s")
                    # ç¡®ä¿å‚æ•°ç±»å‹æ­£ç¡®ï¼Œç§»é™¤å¯èƒ½å­˜åœ¨çš„é¢å¤–å¼•å·
                    if isinstance(value, str):
                        # ç§»é™¤å¯èƒ½å­˜åœ¨çš„é¦–å°¾å¼•å·
                        cleaned_value = value.strip().strip("'").strip('"')
                        params.append(cleaned_value)
                    else:
                        params.append(value)

            # æ·»åŠ æ—¶é—´æ¡ä»¶ï¼šcreated < archive_date_threshold
            if archive_date_threshold is not None:
                where_conditions.append("`created` < %s")
                params.append(archive_date_threshold.strftime('%Y-%m-%d %H:%M:%S'))

            if not where_conditions:
                # å¦‚æœæ²¡æœ‰å…¶ä»–æ¡ä»¶ï¼Œåˆ™ç›´æ¥æŸ¥è¯¢æœ€å°ID
                dynamic_query = f"SELECT MIN(id) as min_id FROM `{table_name}`"
                params = []
            else:
                # æ„å»ºå¸¦æœ‰WHEREæ¡ä»¶çš„æŸ¥è¯¢
                where_clause = " AND ".join(where_conditions)
                dynamic_query = f"SELECT MIN(id) as min_id FROM `{table_name}` WHERE {where_clause}"

            try:
                # æ‰§è¡Œå‰å…ˆè¿›è¡Œè°ƒè¯•æŸ¥è¯¢ï¼Œç¡®ä¿å‚æ•°ä¼ é€’æ­£ç¡®
                logger.info(f"  æ‰§è¡ŒæŸ¥è¯¢: {dynamic_query}  å‚æ•°: {params}")

                # å°è¯•æ‰‹åŠ¨æ„å»ºSQLæŸ¥è¯¢ç”¨äºè°ƒè¯•ï¼ˆä»…ç”¨äºè°ƒè¯•ç›®çš„ï¼Œä¸æ‰§è¡Œï¼‰
                debug_query = dynamic_query
                for param in params:
                    if isinstance(param, str):
                        debug_query = debug_query.replace('%s', f"'{param}'", 1)  # PyMySQLä¼šè‡ªåŠ¨å¤„ç†å¼•å·
                    else:
                        debug_query = debug_query.replace('%s', str(param), 1)
                logger.info(f"  è°ƒè¯•ç”¨çš„å®é™…æŸ¥è¯¢: {debug_query}")

                db_cursor.execute(dynamic_query, params)
                result = db_cursor.fetchone()

                # æ·»åŠ ç»“æœè°ƒè¯•ä¿¡æ¯
                logger.info(f"  æŸ¥è¯¢ç»“æœ: {result}")
                if result:
                    logger.info(f"  ç»“æœé•¿åº¦: {len(result)}, ç¬¬ä¸€ä¸ªå…ƒç´ : {result[0] if len(result) > 0 else 'N/A'}")

                if result and result[0] is not None:
                    min_id = int(result[0])
                    logger.info(f"  è¡¨ {table_name} ä¸­æ»¡è¶³æ¡ä»¶çš„æœ€å°IDä¸º: {min_id}")

                    # æ›´æ–°æˆ–æ’å…¥ id < X çš„è§„åˆ™
                    # æ£€æŸ¥ ttx_archive_rule_term ä¸­æ˜¯å¦å·²æœ‰ id < X çš„è§„åˆ™
                    check_sql = "SELECT value FROM ttx_archive_rule_term WHERE headerId=%s AND field='id' AND operator='<';"
                    db_cursor.execute(check_sql, (header_id,))
                    existing_result = db_cursor.fetchone()

                    if existing_result:
                        # å¦‚æœå·²æœ‰è§„åˆ™ï¼Œæ›´æ–°ä¸ºæœ€å°ID
                        update_sql = "UPDATE ttx_archive_rule_term SET `value`=%s WHERE headerId=%s AND field='id' AND operator='<';"
                        db_cursor.execute(update_sql, (min_id, header_id))
                        affected_rows = db_cursor.rowcount
                        logger.info(f"    âœ“ æ›´æ–°äº† {affected_rows} æ¡è®°å½•ï¼Œvalue è®¾ç½®ä¸ºæœ€å°ID {min_id}")
                    else:
                        # å¦‚æœæ²¡æœ‰è§„åˆ™ï¼Œæ’å…¥æ–°çš„è§„åˆ™
                        insert_sql = """
                        INSERT INTO ttx_archive_rule_term (headerId, field, operator, value, created, lastUpdated, createdBy, lastUpdatedBy)
                        VALUES (%s, 'id', '<', %s, NOW(), NOW(), 'INIT_SYSTEM', 'INIT_SYSTEM');
                        """
                        db_cursor.execute(insert_sql, (header_id, min_id))
                        affected_rows = db_cursor.rowcount
                        logger.info(f"    âœ“ æ’å…¥äº† {affected_rows} æ¡è®°å½•ï¼Œvalue è®¾ç½®ä¸ºæœ€å°ID {min_id}")
                else:
                    # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦å…ˆæ£€æŸ¥æ˜¯å¦æœ‰æ»¡è¶³æ¡ä»¶çš„è®°å½•å­˜åœ¨
                    # é‡æ–°æ„å»ºæŸ¥è¯¢æ¥æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ»¡è¶³æ¡ä»¶çš„è®°å½•
                    if not where_conditions:
                        count_query = f"SELECT COUNT(*) as count FROM `{table_name}`"
                        count_params = []
                    else:
                        count_query = f"SELECT COUNT(*) as count FROM `{table_name}` WHERE {where_clause}"
                        count_params = params

                    logger.info(f"  æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ»¡è¶³æ¡ä»¶çš„è®°å½•: {count_query}  å‚æ•°: {count_params}")
                    db_cursor.execute(count_query, count_params)
                    count_result = db_cursor.fetchone()

                    if count_result and count_result[0] > 0:
                        logger.warning(
                            f"  æ£€æµ‹åˆ°å­˜åœ¨æ»¡è¶³æ¡ä»¶çš„è®°å½•({count_result[0]}æ¡)ï¼Œä½†MIN(id)ä¸ºNULLï¼Œå¯èƒ½å­˜åœ¨ç©ºå€¼æˆ–ç‰¹æ®Šæ•°æ®ç±»å‹")

                        # æ·»åŠ é¢å¤–çš„è°ƒè¯•æŸ¥è¯¢æ¥ç¡®è®¤æ•°æ®ç¡®å®å­˜åœ¨
                        debug_where_clause = " AND ".join(where_conditions)
                        debug_query = f"SELECT id, created FROM `{table_name}` WHERE {debug_where_clause} LIMIT 5"
                        logger.info(f"  è°ƒè¯•æŸ¥è¯¢: {debug_query}  å‚æ•°: {params}")
                        # é‡æ–°æ‰§è¡Œè°ƒè¯•æŸ¥è¯¢ï¼Œç¡®ä¿å‚æ•°å¤„ç†æ­£ç¡®
                        db_cursor.execute(debug_query, params)
                        debug_results = db_cursor.fetchall()
                        logger.info(f"  è°ƒè¯•ç»“æœ: {debug_results}")

                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®ç±»å‹é—®é¢˜
                        id_values_query = f"SELECT id FROM `{table_name}` WHERE {where_clause} AND id IS NOT NULL ORDER BY id ASC LIMIT 10"
                        logger.info(f"  IDå€¼æ£€æŸ¥æŸ¥è¯¢: {id_values_query}  å‚æ•°: {params}")
                        db_cursor.execute(id_values_query, params)
                        id_results = db_cursor.fetchall()
                        logger.info(f"  IDå€¼ç»“æœ: {id_results}")

                        # å°è¯•æŸ¥è¯¢æ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„IDå¹¶æ‰¾æœ€å°å€¼
                        all_ids_query = f"SELECT id FROM `{table_name}`"
                        if where_conditions:
                            all_ids_query += f" WHERE {where_clause}"
                        all_ids_query += " AND id IS NOT NULL ORDER BY id ASC LIMIT 1"

                        logger.info(f"  å°è¯•æŸ¥è¯¢éç©ºIDçš„æœ€å°å€¼: {all_ids_query}  å‚æ•°: {params}")
                        db_cursor.execute(all_ids_query, params)
                        all_ids_result = db_cursor.fetchone()

                        if all_ids_result and all_ids_result[0] is not None:
                            min_id = int(all_ids_result[0])
                            logger.info(f"  æˆåŠŸæ‰¾åˆ°éç©ºæœ€å°ID: {min_id}")

                            # æ›´æ–°æˆ–æ’å…¥ id < X çš„è§„åˆ™
                            check_sql = "SELECT value FROM ttx_archive_rule_term WHERE headerId=%s AND field='id' AND operator='<';"
                            db_cursor.execute(check_sql, (header_id,))
                            existing_result = db_cursor.fetchone()

                            if existing_result:
                                update_sql = "UPDATE ttx_archive_rule_term SET `value`=%s WHERE headerId=%s AND field='id' AND operator='<';"
                                db_cursor.execute(update_sql, (min_id, header_id))
                                affected_rows = db_cursor.rowcount
                                logger.info(f"    âœ“ æ›´æ–°äº† {affected_rows} æ¡è®°å½•ï¼Œvalue è®¾ç½®ä¸ºæœ€å°ID {min_id}")
                            else:
                                insert_sql = """
                                INSERT INTO ttx_archive_rule_term (headerId, field, operator, value, created, lastUpdated, createdBy, lastUpdatedBy)
                                VALUES (%s, 'id', '<', %s, NOW(), NOW(), 'INIT_SYSTEM', 'INIT_SYSTEM');
                                """
                                db_cursor.execute(insert_sql, (header_id, min_id))
                                affected_rows = db_cursor.rowcount
                                logger.info(f"    âœ“ æ’å…¥äº† {affected_rows} æ¡è®°å½•ï¼Œvalue è®¾ç½®ä¸ºæœ€å°ID {min_id}")
                        else:
                            logger.warning(f"  ä»ç„¶æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„IDå€¼ï¼Œè®¾ç½®é»˜è®¤å€¼{DEFAULT_MIN_ID_VALUE}")

                            # æ£€æŸ¥ ttx_archive_rule_term ä¸­æ˜¯å¦å·²æœ‰ id < X çš„è§„åˆ™
                            check_sql = "SELECT value FROM ttx_archive_rule_term WHERE headerId=%s AND field='id' AND operator='<';"
                            db_cursor.execute(check_sql, (header_id,))
                            existing_result = db_cursor.fetchone()

                            if existing_result:
                                # å¦‚æœå·²æœ‰è§„åˆ™ï¼Œæ›´æ–°ä¸ºé»˜è®¤å€¼
                                update_sql = f"UPDATE ttx_archive_rule_term SET `value`={DEFAULT_MIN_ID_VALUE} WHERE headerId=%s AND field='id' AND operator='<';"
                                db_cursor.execute(update_sql, (header_id,))
                                affected_rows = db_cursor.rowcount
                                logger.info(
                                    f"    âœ“ æ›´æ–°äº† {affected_rows} æ¡è®°å½•ï¼Œvalue è®¾ç½®ä¸ºé»˜è®¤å€¼ {DEFAULT_MIN_ID_VALUE}")
                            else:
                                # å¦‚æœæ²¡æœ‰è§„åˆ™ï¼Œæ’å…¥æ–°çš„è§„åˆ™
                                insert_sql = """
                                INSERT INTO ttx_archive_rule_term (headerId, field, operator, value, created, lastUpdated, createdBy, lastUpdatedBy)
                                VALUES (%s, 'id', '<', %s, NOW(), NOW(), 'INIT_SYSTEM', 'INIT_SYSTEM');
                                """
                                db_cursor.execute(insert_sql, (header_id, DEFAULT_MIN_ID_VALUE))
                                affected_rows = db_cursor.rowcount
                                logger.info(
                                    f"    âœ“ æ’å…¥äº† {affected_rows} æ¡è®°å½•ï¼Œvalue è®¾ç½®ä¸ºé»˜è®¤å€¼ {DEFAULT_MIN_ID_VALUE}")
                    else:
                        logger.info(
                            f"  ç¡®è®¤è¡¨ {table_name} ä¸­ç¡®å®æ²¡æœ‰æ»¡è¶³æ¡ä»¶çš„è®°å½•({count_result[0]}æ¡)ï¼Œè®¾ç½®é»˜è®¤å€¼{DEFAULT_MIN_ID_VALUE}")

                        # æ£€æŸ¥ ttx_archive_rule_term ä¸­æ˜¯å¦å·²æœ‰ id < X çš„è§„åˆ™
                        check_sql = "SELECT value FROM ttx_archive_rule_term WHERE headerId=%s AND field='id' AND operator='<';"
                        db_cursor.execute(check_sql, (header_id,))
                        existing_result = db_cursor.fetchone()

                        if existing_result:
                            # å¦‚æœå·²æœ‰è§„åˆ™ï¼Œæ›´æ–°ä¸ºé»˜è®¤å€¼
                            update_sql = f"UPDATE ttx_archive_rule_term SET `value`={DEFAULT_MIN_ID_VALUE} WHERE headerId=%s AND field='id' AND operator='<';"
                            db_cursor.execute(update_sql, (header_id,))
                            affected_rows = db_cursor.rowcount
                            logger.info(
                                f"    âœ“ æ›´æ–°äº† {affected_rows} æ¡è®°å½•ï¼Œvalue è®¾ç½®ä¸ºé»˜è®¤å€¼ {DEFAULT_MIN_ID_VALUE}")
                        else:
                            # å¦‚æœæ²¡æœ‰è§„åˆ™ï¼Œæ’å…¥æ–°çš„è§„åˆ™
                            insert_sql = """
                            INSERT INTO ttx_archive_rule_term (headerId, field, operator, value, created, lastUpdated, createdBy, lastUpdatedBy)
                            VALUES (%s, 'id', '<', %s, NOW(), NOW(), 'INIT_SYSTEM', 'INIT_SYSTEM');
                            """
                            db_cursor.execute(insert_sql, (header_id, DEFAULT_MIN_ID_VALUE))
                            affected_rows = db_cursor.rowcount
                            logger.info(
                                f"    âœ“ æ’å…¥äº† {affected_rows} æ¡è®°å½•ï¼Œvalue è®¾ç½®ä¸ºé»˜è®¤å€¼ {DEFAULT_MIN_ID_VALUE}")

            except pymysql.Error as e:
                logger.error(f"  æŸ¥è¯¢è¡¨ {table_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue  # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè¡¨

        # æäº¤äº‹åŠ¡ä»¥ç¡®ä¿æ›´æ”¹ç”Ÿæ•ˆ
        db_connection.commit()
        logger.info(f"  âœ“ æ•°æ®åº“äº‹åŠ¡æäº¤æˆåŠŸ")

        logger.info(f"\n{'=' * 70}")
        logger.info(f"ğŸ“Š åˆå§‹åŒ–ä»»åŠ¡å®Œæˆ")
        logger.info(f"{'=' * 70}")
        logger.info(f"ğŸ‰ æ‰€æœ‰è¡¨çš„å½’æ¡£è§„åˆ™å·²æ ¹æ®å…¶æ•°æ®è¡¨ä¸­çš„æ¡ä»¶æŸ¥è¯¢ç»“æœè®¾ç½®äº†æœ€å°ID")
        logger.info(f"{'=' * 70}")

    except pymysql.Error as e:
        logger.error(f"æ•°æ®åº“æ“ä½œé”™è¯¯: {e}")
    except redis.ConnectionError as e:
        logger.error(f"Redis è¿æ¥é”™è¯¯: {e}")
    except Exception as e:
        logger.error(f"è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")  # è®°å½•å®Œæ•´çš„å †æ ˆè·Ÿè¸ª
    finally:
        # å…³é—­æ•°æ®åº“è¿æ¥
        if db_connection:
            try:
                db_cursor.close()
                db_connection.close()
                logger.info("âœ“ æ•°æ®åº“è¿æ¥å·²å…³é—­")
            except Exception as e:
                logger.error(f"å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        # å…³é—­ Redis è¿æ¥
        if redis_client:
            try:
                logger.info("âœ“ Redis è¿æ¥å·²å¤„ç†")
            except Exception as e:
                logger.error(f"å¤„ç† Redis è¿æ¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")


def update_and_request():
    """
    ä¸»å¾ªç¯å‡½æ•°ï¼šæŸ¥è¯¢è¡¨å¤´ä¿¡æ¯ï¼Œæ›´æ–°å½’æ¡£è§„åˆ™å€¼ï¼Œè¯·æ±‚APIã€å¹¶ç­‰å¾…å½’æ¡£ä»»åŠ¡å®Œæˆ
    """
    db_connection = None
    redis_client = None
    execution_times = []  # å­˜å‚¨æ¯æ¬¡å½’æ¡£æ‰§è¡Œçš„æ—¶é—´

    try:
        # è¿æ¥æ•°æ®åº“
        logger.info("æ­£åœ¨è¿æ¥æ•°æ®åº“...")
        db_connection = pymysql.connect(**DB_CONFIG)
        db_cursor = db_connection.cursor()

        # è¿æ¥ Redis
        logger.info("æ­£åœ¨è¿æ¥ Redis...")
        redis_client = redis.Redis(**REDIS_CONFIG)
        # å°è¯•æ‰§è¡Œä¸€ä¸ªç®€å•çš„ Redis å‘½ä»¤æ¥æµ‹è¯•è¿æ¥
        redis_client.ping()
        logger.info("Redis è¿æ¥æˆåŠŸã€‚")

        # æŸ¥è¯¢æ‰€æœ‰ autoArchive=1 çš„è¡¨å¤´ä¿¡æ¯ï¼ŒåŒ…æ‹¬å½’æ¡£å¤©æ•°è®¾ç½®
        logger.info("æ­£åœ¨æŸ¥è¯¢ ttx_archive_rule_header è¡¨ä¸­ autoArchive=1 çš„è®°å½•...")
        query_sql = "SELECT id, tableName, archiveDaysBefore FROM ttx_archive_rule_header WHERE autoArchive=1"
        db_cursor.execute(query_sql)
        table_records = db_cursor.fetchall()

        if not table_records:
            logger.warning("æœªæ‰¾åˆ° autoArchive=1 çš„è¡¨è®°å½•ï¼Œç¨‹åºé€€å‡ºã€‚")
            return

        logger.info(f"å…±æŸ¥è¯¢åˆ° {len(table_records)} ä¸ªéœ€è¦å½’æ¡£çš„è¡¨:")
        for record in table_records:
            logger.info(f"  - ID: {record[0]}, TableName: {record[1]}, ArchiveDaysBefore: {record[2]}")

        current_iteration = 0

        for iteration in range(total_iterations):
            current_iteration += 1
            progress_percentage = (current_iteration / total_iterations) * 100

            # 1. æ£€æŸ¥ Redis é”ï¼Œç­‰å¾…å½’æ¡£ä»»åŠ¡å®Œæˆ
            logger.info(f"[{progress_percentage:.1f}%] æ£€æŸ¥ Redis é” {LOCK_KEY} æ˜¯å¦å­˜åœ¨ï¼Œä»¥ç¡®å®šå½’æ¡£ä»»åŠ¡æ˜¯å¦ä»åœ¨æ‰§è¡Œ...")
            lock_check_start_time = time.time()
            while True:
                if redis_client.exists(LOCK_KEY):
                    elapsed_time = int(time.time() - lock_check_start_time)
                    logger.info(
                        f"    é” {LOCK_KEY} å­˜åœ¨ï¼Œå½’æ¡£ä»»åŠ¡ä»åœ¨æ‰§è¡Œä¸­ã€‚å·²ç­‰å¾… {elapsed_time}sï¼Œç»§ç»­ç­‰å¾… {WAIT_SECONDS_FOR_LOCK_CHECK} ç§’åé‡è¯•...")
                    time.sleep(WAIT_SECONDS_FOR_LOCK_CHECK)
                else:
                    elapsed_time = int(time.time() - lock_check_start_time)
                    logger.info(f"    é” {LOCK_KEY} ä¸å­˜åœ¨ï¼Œå½’æ¡£ä»»åŠ¡å·²ç»“æŸã€‚ç­‰å¾…äº† {elapsed_time} ç§’ã€‚")
                    break

            logger.info(f"\n{'=' * 60}")
            logger.info(
                f"å¤„ç†è¿›åº¦: [{current_iteration}/{total_iterations}] | å½“å‰è¿­ä»£: {iteration} | å®Œæˆç‡: {progress_percentage:.1f}%")
            logger.info(f"{'=' * 60}")

            # è®°å½•å½’æ¡£å¼€å§‹æ—¶é—´
            archive_start_time = time.time()
            logger.info(
                f"  ğŸ• å½’æ¡£ä»»åŠ¡å¼€å§‹æ—¶é—´: {datetime.fromtimestamp(archive_start_time).strftime('%Y-%m-%d %H:%M:%S')}")

            # 2. æŸ¥è¯¢å½“å‰æ‰€æœ‰è¡¨å¤´IDå¯¹åº”çš„ç°æœ‰valueå€¼ï¼Œå¹¶é€’å¢{ARCHIVE_INCREMENT_VALUE}
            for header_id, table_name, archive_days_before in table_records:
                # å…ˆæŸ¥è¯¢å½“å‰çš„valueå€¼
                select_sql = "SELECT value FROM ttx_archive_rule_term WHERE headerId=%s AND field='id' AND operator='<';"
                db_cursor.execute(select_sql, (header_id,))
                result = db_cursor.fetchone()

                if result:
                    current_value = int(result[0])
                    new_value = current_value + ARCHIVE_INCREMENT_VALUE
                    logger.info(f"  å¤„ç†è¡¨ {table_name} (ID: {header_id}): å½“å‰å€¼ {current_value}, æ›´æ–°ä¸º {new_value}")

                    # æ‰§è¡Œæ›´æ–°
                    update_sql = "UPDATE ttx_archive_rule_term SET `value`=%s WHERE headerId=%s AND field='id' AND operator='<';"
                    db_cursor.execute(update_sql, (new_value, header_id))
                    affected_rows = db_cursor.rowcount
                    logger.info(f"    âœ“ æ›´æ–°äº† {affected_rows} æ¡è®°å½•ï¼Œvalue è®¾ç½®ä¸º {new_value}")
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è§„åˆ™ï¼Œæ’å…¥é»˜è®¤å€¼{DEFAULT_MIN_ID_VALUE}
                    logger.info(
                        f"  è­¦å‘Š: è¡¨ {table_name} (ID: {header_id}) æ²¡æœ‰æ‰¾åˆ° field='id' ä¸” operator='<' çš„è§„åˆ™ï¼Œæ’å…¥é»˜è®¤å€¼{DEFAULT_MIN_ID_VALUE}")

                    # æ’å…¥æ–°çš„è§„åˆ™
                    insert_sql = """
                    INSERT INTO ttx_archive_rule_term (headerId, field, operator, value, created, lastUpdated, createdBy, lastUpdatedBy)
                    VALUES (%s, 'id', '<', %s, NOW(), NOW(), 'SYSTEM', 'SYSTEM')
                    ON DUPLICATE KEY UPDATE value = %s, lastUpdated = NOW(), lastUpdatedBy = 'SYSTEM';
                    """
                    db_cursor.execute(insert_sql, (header_id, DEFAULT_MIN_ID_VALUE, DEFAULT_MIN_ID_VALUE))
                    affected_rows = db_cursor.rowcount
                    logger.info(f"    âœ“ æ’å…¥äº† {affected_rows} æ¡è®°å½•ï¼Œvalue è®¾ç½®ä¸º {DEFAULT_MIN_ID_VALUE}")

            # æäº¤äº‹åŠ¡ä»¥ç¡®ä¿æ›´æ”¹ç”Ÿæ•ˆ
            db_connection.commit()
            logger.info(f"  âœ“ æ•°æ®åº“äº‹åŠ¡æäº¤æˆåŠŸ")

            # 3. è¯·æ±‚ API
            logger.info(f"  â†’ æ­£åœ¨è¯·æ±‚ API: {API_URL}")
            api_start_time = time.time()
            try:
                # ä½¿ç”¨ä¼šè¯å¯¹è±¡ï¼Œæ”¯æŒé•¿è¿æ¥
                response = session.post(API_URL, timeout=30)
                api_end_time = time.time()
                api_duration = api_end_time - api_start_time
                logger.info(f"  â† API è¯·æ±‚å®Œæˆï¼ŒçŠ¶æ€ç : {response.status_code}ï¼Œè€—æ—¶: {api_duration:.2f}ç§’")

                # æ ¹æ®æ‚¨çš„ API æ–‡æ¡£åˆ¤æ–­æˆåŠŸä¸å¦
                if response.status_code == 200:
                    logger.info(f"  âœ“ API è¯·æ±‚æˆåŠŸ")
                else:
                    logger.warning(f"  âš ï¸  API è¯·æ±‚è¿”å›é200çŠ¶æ€ç : {response.status_code}")

                # å¯é€‰ï¼šè®°å½•å“åº”å†…å®¹ï¼ˆå¦‚æœéœ€è¦è°ƒè¯•ï¼‰
                # logger.debug(f"    å“åº”å†…å®¹: {response.text[:200]}...")  # åªè®°å½•å‰200å­—ç¬¦

            except requests.exceptions.Timeout:
                logger.error(f"  âŒ API è¯·æ±‚è¶…æ—¶ (30ç§’)")
            except requests.exceptions.ConnectionError:
                logger.error(f"  âŒ API è¿æ¥é”™è¯¯")
            except requests.exceptions.RequestException as e:
                logger.error(f"  âŒ API è¯·æ±‚å‘ç”Ÿé”™è¯¯: {e}")
                # å¦‚æœAPIå‡ºé”™ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æš‚åœæˆ–é€€å‡ºï¼Œè¿™é‡Œåªæ˜¯æ‰“å°é”™è¯¯ç»§ç»­å¾ªç¯
                # raise # å–æ¶ˆæ³¨é‡Šè¿™è¡Œå¯ä»¥è®©è„šæœ¬åœ¨æ­¤å¤„åœæ­¢

            # ç­‰å¾…å½’æ¡£ä»»åŠ¡å®Œæˆ
            logger.info(f"  ğŸ”„ ç­‰å¾…å½’æ¡£ä»»åŠ¡å®Œæˆ...")
            archive_wait_start = time.time()
            while True:
                if redis_client.exists(LOCK_KEY):
                    time.sleep(WAIT_SECONDS_FOR_LOCK_CHECK)
                else:
                    break
            archive_wait_duration = time.time() - archive_wait_start
            logger.info(f"  âœ… å½’æ¡£ä»»åŠ¡å·²å®Œæˆï¼Œç­‰å¾…è€—æ—¶: {archive_wait_duration:.2f}ç§’")

            # è®¡ç®—æœ¬æ¬¡å½’æ¡£çš„æ€»è€—æ—¶
            archive_total_duration = time.time() - archive_start_time
            execution_times.append({
                'iteration': iteration,
                'duration': archive_total_duration,
                'start_time': datetime.fromtimestamp(archive_start_time),
                'end_time': datetime.fromtimestamp(time.time())
            })

            logger.info(f"  ğŸ“Š æœ¬æ¬¡å½’æ¡£æ€»è€—æ—¶: {archive_total_duration:.2f}ç§’ ({archive_total_duration / 60:.2f}åˆ†é’Ÿ)")
            logger.info(
                f"  ğŸ“… å½’æ¡£æ—¶é—´æ®µ: {datetime.fromtimestamp(archive_start_time).strftime('%H:%M:%S')} -> {datetime.fromtimestamp(time.time()).strftime('%H:%M:%S')}")

        # è¾“å‡ºç»Ÿè®¡æ‘˜è¦
        logger.info(f"\n{'=' * 70}")
        logger.info(f"ğŸ“Š å½’æ¡£ä»»åŠ¡æ‰§è¡Œç»Ÿè®¡æ‘˜è¦")
        logger.info(f"{'=' * 70}")

        if execution_times:
            total_duration = sum(item['duration'] for item in execution_times)
            avg_duration = total_duration / len(execution_times)
            max_duration = max(execution_times, key=lambda x: x['duration'])
            min_duration = min(execution_times, key=lambda x: x['duration'])

            logger.info(f"æ€»æ‰§è¡Œæ¬¡æ•°: {len(execution_times)}")
            logger.info(f"æ€»è€—æ—¶: {total_duration:.2f}ç§’ ({total_duration / 60:.2f}åˆ†é’Ÿ)")
            logger.info(f"å¹³å‡è€—æ—¶: {avg_duration:.2f}ç§’ ({avg_duration / 60:.2f}åˆ†é’Ÿ)")
            logger.info(f"æœ€é•¿è€—æ—¶: {max_duration['duration']:.2f}ç§’ (è¿­ä»£: {max_duration['iteration']})")
            logger.info(f"æœ€çŸ­è€—æ—¶: {min_duration['duration']:.2f}ç§’ (è¿­ä»£: {min_duration['iteration']})")

        logger.info(f"{'=' * 70}")
        logger.info(f"ğŸ‰ æ‰€æœ‰å¾ªç¯æ‰§è¡Œå®Œæ¯•ï¼æ€»è®¡å¤„ç†äº† {total_iterations} æ¬¡è¿­ä»£")
        logger.info(f"{'=' * 70}")

    except pymysql.Error as e:
        logger.error(f"æ•°æ®åº“æ“ä½œé”™è¯¯: {e}")
    except redis.ConnectionError as e:
        logger.error(f"Redis è¿æ¥é”™è¯¯: {e}")
    except Exception as e:
        logger.error(f"è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")  # è®°å½•å®Œæ•´çš„å †æ ˆè·Ÿè¸ª
    finally:
        # å…³é—­æ•°æ®åº“è¿æ¥
        if db_connection:
            try:
                db_cursor.close()
                db_connection.close()
                logger.info("âœ“ æ•°æ®åº“è¿æ¥å·²å…³é—­")
            except Exception as e:
                logger.error(f"å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        # å…³é—­ Redis è¿æ¥
        if redis_client:
            try:
                logger.info("âœ“ Redis è¿æ¥å·²å¤„ç†")
            except Exception as e:
                logger.error(f"å¤„ç† Redis è¿æ¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    start_time = time.time()
    logger.info("=" * 70)
    logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œ WMS å½’æ¡£ä»»åŠ¡ç®¡ç†è„šæœ¬")
    logger.info("ğŸ“‹ è„šæœ¬å°†å…ˆè¿›è¡Œåˆå§‹åŒ–ï¼Œç„¶åç»Ÿè®¡æ¯æ¬¡å½’æ¡£ä»»åŠ¡çš„æ‰§è¡Œæ—¶é—´")
    logger.info("=" * 70)

    # æ£€æŸ¥APIæ˜¯å¦æ”¯æŒé•¿è¿æ¥
    logger.info("\n--- æ£€æŸ¥APIé•¿è¿æ¥æ”¯æŒæƒ…å†µ ---")
    is_long_connection_supported = check_long_connection_support(API_URL)
    if is_long_connection_supported:
        logger.info("âœ… API æ”¯æŒé•¿è¿æ¥ï¼Œå°†ä½¿ç”¨è¿æ¥æ± å’Œä¼šè¯å¤ç”¨ä¼˜åŒ–æ€§èƒ½")
    else:
        logger.warning("âš ï¸  API å¯èƒ½ä¸æ”¯æŒé•¿è¿æ¥ï¼Œä½†ä»å°†å°è¯•ä½¿ç”¨è¿æ¥æ± ")

    # å…ˆæ‰§è¡Œåˆå§‹åŒ–
    logger.info("\n--- å¼€å§‹æ‰§è¡Œåˆå§‹åŒ–æ­¥éª¤ ---")
    initialize_and_update()
    logger.info("\n--- åˆå§‹åŒ–æ­¥éª¤å®Œæˆï¼Œå¼€å§‹æ‰§è¡Œå½’æ¡£ä»»åŠ¡ ---\n")

    # å†æ‰§è¡Œå½’æ¡£ä»»åŠ¡
    update_and_request()

    end_time = time.time()
    duration = end_time - start_time
    logger.info("=" * 70)
    logger.info(f"ğŸ è„šæœ¬æ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶: {duration:.2f} ç§’ ({duration / 60:.2f} åˆ†é’Ÿ)")
    logger.info("=" * 70)
